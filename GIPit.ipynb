{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "3UWM4wr4O490"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qemXeBEuTGZO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 1.  Install relevant packages\n",
        "# You may need to install more packages depending on your current list of packages\n",
        "\n",
        "pip install gip-bio\n",
        "pip install multiprocess\n",
        "pip install openpyxl\n",
        "pip install igraph\n",
        "\n",
        "#pip list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbVdZFV8TsmO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "2. If you are using Google Colab then mount your google drive and then change directory\n",
        "to the relevant folder (change path to match your directory layout)\n",
        "Otherwise, just change directory to the folder with relevant input files\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cd /add/your/path/here"
      ],
      "metadata": {
        "id": "zPpou0bY-Pbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/GIP/GitHub"
      ],
      "metadata": {
        "id": "C8tY7kKIQ1oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-processing**"
      ],
      "metadata": {
        "id": "Uc8j7RBVWzDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "3. GIP cannot handle grouped identifiers, this code block creates a version of\n",
        "the Abundance sheet that only retains the first identifier in any grouping, as\n",
        "well as a lookup table that will be used later\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data from the Abundances file\n",
        "benchmark = pd.read_csv('Input/Abundances.tsv', sep='\\t')\n",
        "\n",
        "# Initialize an empty list for the lookup table\n",
        "lookup_table = []\n",
        "\n",
        "# Function to process each row in the 'Protein IDs' column\n",
        "def process_protein_ids(row):\n",
        "    # Retrieve the 'Protein IDs' value for the current row\n",
        "    protein_ids = row['Protein IDs']\n",
        "\n",
        "    # Split the protein IDs by semicolon to handle multiple names\n",
        "    protein_names = protein_ids.split(';')\n",
        "\n",
        "    # Get the first protein name\n",
        "    first_protein_name = protein_names[0]\n",
        "\n",
        "    # Add to the lookup table with first name and full group of names\n",
        "    lookup_table.append({\n",
        "        'First Name': first_protein_name,\n",
        "        'Full Grouped Names': protein_ids\n",
        "    })\n",
        "\n",
        "    # Replace the current row's Protein ID with just the first name\n",
        "    return first_protein_name\n",
        "\n",
        "# Apply the function to each row in the 'Protein IDs' column\n",
        "benchmark['Protein IDs'] = benchmark.apply(process_protein_ids, axis=1)\n",
        "\n",
        "# Convert the lookup table to a DataFrame\n",
        "lookup_df = pd.DataFrame(lookup_table)\n",
        "\n",
        "# Save the modified benchmark table with only the first protein name\n",
        "benchmark.to_csv('Input/Abundances_modified.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Save the lookup table (first names and full grouped names)\n",
        "lookup_df.to_csv('Input/protein_lookup_table.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(\"Files saved: 'Abundances_modified.tsv' and 'protein_lookup_table.tsv to the Input folder'\")\n"
      ],
      "metadata": {
        "id": "mbAZM6SsIVq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calibration**"
      ],
      "metadata": {
        "id": "4qha6UNePG_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "4.1. Calibrate your MW across slices by adding values of known slices to X and Y variables,\n",
        "and plot them. For a good calibration please ensure that first and last slices are\n",
        "represented in the X and Y variables. Also number of slices in the commented line below\n",
        "if your dataset has has more or less than 48 slices\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Original slice numbers and molecular weights in Daltons\n",
        "X = np.array([8, 18, 23, 29, 35, 39]).reshape(-1, 1)  # Slice number\n",
        "Y = np.array([1048000, 720000, 480000, 242000, 146000, 66000])  # Molecular weight in Daltons\n",
        "\n",
        "# Apply base-10 log transformation to the molecular weight data\n",
        "Y_log10 = np.log10(Y)\n",
        "\n",
        "# Fit linear regression on the log-transformed data\n",
        "model = LinearRegression().fit(X, Y_log10)\n",
        "\n",
        "# Predict log-transformed molecular weights for all slices from 1 to 48\n",
        "X_all = np.arange(1, 49).reshape(-1, 1) # In np.arange(1, x) x should be number of slices in your dataset+1\n",
        "Y_pred_log10 = model.predict(X_all)\n",
        "\n",
        "# Revert predictions back to the original scale (10** for base-10) and convert to kilodaltons\n",
        "Y_pred_kDa = (10 ** Y_pred_log10) / 1000  # Convert to kilodaltons\n",
        "\n",
        "# Print slice number and predicted molecular weight in kilodaltons\n",
        "print(\"Predicted Molecular Weights (kDa) for Slices 1 to 48:\")\n",
        "for slice_num, mw in zip(X_all.flatten(), Y_pred_kDa):\n",
        "    print(f\"Slice {slice_num}: {mw:.2f} kDa\")\n",
        "\n",
        "# Plotting the original data with log10-linear trendline using Plotly Express\n",
        "df = pd.DataFrame({'Slice': X.flatten(), 'Log10_Molecular_Weight': Y_log10})\n",
        "fig = px.scatter(df, x=\"Slice\", y=\"Log10_Molecular_Weight\", trendline=\"ols\",\n",
        "                 title=\"Log10-Transformed Molecular Weight vs Slice Number (Linear Fit)\")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "FOk2hPacSkpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2. Calibrate your MW across slices by adding values of known slices to X and Y variables, and apply these values to your abundances sheet. The modified sheet's name can be changed in the final part of this block\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Original slice numbers and molecular weights in Daltons\n",
        "X = np.array([8, 18, 23, 29, 35, 39]).reshape(-1, 1)  # Slice number\n",
        "Y = np.array([1048000, 720000, 480000, 242000, 146000, 66000])  # Molecular weight in Daltons\n",
        "\n",
        "# Ask user if the first slice represents the highest or lowest molecular weight\n",
        "user_input = input(\"Does the first slice represent the highest molecular weight (1) or the lowest (2)? Enter 1 or 2: \")\n",
        "if user_input == \"1\":\n",
        "    order_high_to_low = True\n",
        "elif user_input == \"2\":\n",
        "    order_high_to_low = False\n",
        "else:\n",
        "    raise ValueError(\"Invalid input. Please enter 1 or 2.\")\n",
        "\n",
        "# Apply base-10 log transformation to the molecular weight data\n",
        "Y_log10 = np.log10(Y)\n",
        "\n",
        "# Fit linear regression on the log-transformed data\n",
        "model = LinearRegression().fit(X, Y_log10)\n",
        "\n",
        "# Predict log-transformed molecular weights for all slices from 1 to 48\n",
        "X_all = np.arange(1, 49).reshape(-1, 1)\n",
        "Y_pred_log10 = model.predict(X_all)\n",
        "\n",
        "# Revert predictions back to the original scale (10** for base-10) and convert to kilodaltons\n",
        "Y_pred_kDa = (10 ** Y_pred_log10) / 1000  # Convert to kilodaltons and ignore decimals\n",
        "\n",
        "# Reverse the order if the first slice represents the lowest MW\n",
        "if not order_high_to_low:\n",
        "    Y_pred_kDa = Y_pred_kDa[::-1]\n",
        "\n",
        "# Load the original `Abundances_modified.tsv` file\n",
        "df_benchmark = pd.read_csv(\"Input/Abundances_modified.tsv\", sep='\\t')\n",
        "\n",
        "# Keep the original first column name\n",
        "first_column_name = df_benchmark.columns[0]\n",
        "\n",
        "# Rename only the numbered slice columns with molecular weight labels\n",
        "new_headers = [first_column_name] + [f\"{slice_num} - {int(mw)} kDa\" for slice_num, mw in zip(X_all.flatten(), Y_pred_kDa)]\n",
        "df_benchmark.columns = new_headers\n",
        "\n",
        "# Save the new file as `Abundances_final.tsv`\n",
        "df_benchmark.to_csv(\"Input/Abundances_final.tsv\", sep='\\t', index=False)\n",
        "\n",
        "print(\"Abundances_final.tsv has been created in the Input folder with updated headers including molecular weights.\")\n"
      ],
      "metadata": {
        "id": "uK1rLFalScdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run GIP**"
      ],
      "metadata": {
        "id": "jvoJsJxLW6s-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhjDTB9-UXBt"
      },
      "outputs": [],
      "source": [
        "# 5. Run GIP\n",
        "\n",
        "from gip.main import main\n",
        "import gip.process_normalise as prn\n",
        "import pandas as pd\n",
        "\n",
        "# parse complexome profile and protein annotation file\n",
        "prof = prn.parse_profile('Input/Abundances_modified.tsv')\n",
        "#annot = pd.read_csv('Annotation.tsv',sep='\\t',index_col=0)\n",
        "\n",
        "# set ratio of clusters relative to number of detected proteins\n",
        "clust_ratio = 0.5\n",
        "\n",
        "# to run a standard run, using 4 threads for the bootstrapping\n",
        "gip_results = main(prof, clust_ratio, annot_df=None, bs_processes=4, clusttable_fn='cluster_table.tsv', membertable_fn = 'mem_table.tsv', pdf_fn='clusters_output.pdf')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Post-processing**"
      ],
      "metadata": {
        "id": "EulWgHXQW-_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Re-add grouped identifiers back to the GIP output (mem_table)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the mem_table and lookup_table\n",
        "mem_table = pd.read_csv('mem_table.tsv', sep='\\t')\n",
        "lookup_table = pd.read_csv('Input/protein_lookup_table.tsv', sep='\\t')\n",
        "\n",
        "# Create a dictionary from the lookup table for fast lookup\n",
        "lookup_dict = pd.Series(lookup_table['Full Grouped Names'].values, index=lookup_table['First Name']).to_dict()\n",
        "\n",
        "# Function to replace the identifier with the full grouped names from the lookup table\n",
        "def replace_with_full_name(identifier):\n",
        "    # If the identifier is in the lookup dictionary, replace it with the full name\n",
        "    return lookup_dict.get(identifier, identifier)\n",
        "\n",
        "# Apply the function to the 'identifier' column in mem_table\n",
        "mem_table['identifier'] = mem_table['identifier'].apply(replace_with_full_name)\n",
        "\n",
        "# Save the modified mem_table to a new file\n",
        "mem_table.to_csv('modified_mem_table.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(\"File saved: 'modified_mem_table.tsv'\")\n"
      ],
      "metadata": {
        "id": "TlsOVhHbQFd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Add alternate identifiers from original data set to the GIP's output (mem_table)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the modified mem table and original Abundances sheet\n",
        "modified_mem_table = pd.read_csv('modified_mem_table.tsv', sep='\\t')\n",
        "original_benchmark = pd.read_excel('Input/ExtendedAbundances.xlsx')\n",
        "\n",
        "# Ask the user for the column header they want to match from the original Abundances sheet\n",
        "selected_column = input(f\"Please enter the column header from 'ExtendedAbundances.xlsx' you want to match and added to the final mem_table: \")\n",
        "\n",
        "# Check if the selected column exists in the original Abundances sheet\n",
        "if selected_column not in original_benchmark.columns:\n",
        "    print(f\"Error: The column '{selected_column}' does not exist in the 'ExtendedAbundances.xlsx' file.\")\n",
        "else:\n",
        "    # Create a dictionary for fast lookup of identifiers in the original Abundances\n",
        "    # The dictionary will map individual identifiers to the corresponding name from the selected column\n",
        "    lookup_dict = {}\n",
        "\n",
        "    # Fill the lookup dictionary by splitting grouped identifiers from the selected column\n",
        "    for idx, row in original_benchmark.iterrows():\n",
        "        # Split identifiers in the selected column by semicolon and strip any whitespace\n",
        "        identifiers = str(row['Protein IDs']).split(';')\n",
        "        for identifier in identifiers:\n",
        "            lookup_dict[identifier.strip()] = row[selected_column]\n",
        "\n",
        "    # Function to map identifiers in mem table using the lookup dictionary\n",
        "    def map_identifiers(row):\n",
        "        # Split the identifiers in the 'identifier' column by semicolon\n",
        "        mem_identifiers = str(row['identifier']).split(';')\n",
        "\n",
        "        # Try to find a match in the lookup_dict\n",
        "        for mem_identifier in mem_identifiers:\n",
        "            mem_identifier = mem_identifier.strip()\n",
        "            if mem_identifier in lookup_dict:\n",
        "                return lookup_dict[mem_identifier]  # Return the matched name (e.g., 'IKBIP')\n",
        "\n",
        "        return None  # Return None if no match is found\n",
        "\n",
        "    # Apply the function to add the new column with matched names\n",
        "    modified_mem_table['Matched Identifiers'] = modified_mem_table.apply(map_identifiers, axis=1)\n",
        "\n",
        "    # Save the final mem table with the new matched identifiers column\n",
        "    modified_mem_table.to_csv('final_mem_table.tsv', sep='\\t', index=False)\n",
        "\n",
        "    print(\"File saved: 'final_mem_table.tsv'\")\n"
      ],
      "metadata": {
        "id": "6RcdTK7DQrbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarize, Lookup, and Plot**"
      ],
      "metadata": {
        "id": "mYQC-MtJXKIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Summarize GIP's run statistics\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the final mem table\n",
        "final_mem_table = pd.read_csv('final_mem_table.tsv', sep='\\t')\n",
        "\n",
        "# Perform analysis on the number of clusters\n",
        "total_clusters = final_mem_table['clust_id'].nunique()\n",
        "\n",
        "# Calculate total number of proteins\n",
        "total_proteins = final_mem_table['identifier'].nunique()\n",
        "\n",
        "# Group by cluster and count the number of proteins in each cluster\n",
        "proteins_per_cluster = final_mem_table.groupby('clust_id')['identifier'].count()\n",
        "\n",
        "# Find the max, min, and average number of proteins per cluster\n",
        "max_proteins_in_cluster = proteins_per_cluster.max()\n",
        "min_proteins_in_cluster = proteins_per_cluster.min()\n",
        "avg_proteins_per_cluster = proteins_per_cluster.mean()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Summary Statistics for Final Mem Table:\")\n",
        "print(f\"----------------------------------------\")\n",
        "print(f\"Total number of clusters: {total_clusters}\")\n",
        "print(f\"Total number of unique proteins: {total_proteins}\")\n",
        "print(f\"Maximum number of proteins in a cluster: {max_proteins_in_cluster}\")\n",
        "print(f\"Minimum number of proteins in a cluster: {min_proteins_in_cluster}\")\n",
        "print(f\"Average number of proteins per cluster: {avg_proteins_per_cluster:.2f}\")\n"
      ],
      "metadata": {
        "id": "JU9sZHVUnFDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. find the cluster number and interactors of a protein of interest\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the final mem table\n",
        "final_mem_table = pd.read_csv('final_mem_table.tsv', sep='\\t')\n",
        "\n",
        "# User can modify this variable to point to the correct identifier column\n",
        "identifier_column = 'Matched Identifiers'  # This is the column that has the protein names/identifiers\n",
        "\n",
        "# Function to look up the protein and its cluster members\n",
        "def lookup_protein(protein_name):\n",
        "    # Find the row with the given protein name\n",
        "    matching_row = final_mem_table[final_mem_table[identifier_column].str.contains(protein_name, na=False)]\n",
        "\n",
        "    if matching_row.empty:\n",
        "        print(f\"Protein '{protein_name}' not found in the {identifier_column} column.\")\n",
        "    else:\n",
        "        # Get the cluster ID for the matched protein\n",
        "        cluster_id = matching_row.iloc[0]['clust_id']\n",
        "\n",
        "        # Find all proteins in the same cluster\n",
        "        cluster_members = final_mem_table[final_mem_table['clust_id'] == cluster_id][identifier_column]\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"Protein '{protein_name}' is in cluster {cluster_id}.\")\n",
        "        print(f\"Other members in the same cluster ({cluster_id}):\")\n",
        "        print(cluster_members.tolist())\n",
        "\n",
        "# Example usage: Call the function with the name of the protein you're looking for\n",
        "protein_to_search = input(\"Enter the name of the protein: \")\n",
        "lookup_protein(protein_to_search)\n"
      ],
      "metadata": {
        "id": "HWT5HlEUnHx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Generate profile plot for a GIP identified cluster\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "import plotly.io as pio\n",
        "\n",
        "# Set Plotly to display inline in notebook (important for Colab)\n",
        "pio.renderers.default = 'colab'\n",
        "\n",
        "# Function to load TSV files and return dataframes\n",
        "def load_data():\n",
        "    final_mem_table = pd.read_csv('final_mem_table.tsv', sep='\\t')\n",
        "    benchmark_modified = pd.read_csv('Input/Abundances_final.tsv', sep='\\t')\n",
        "    return final_mem_table, benchmark_modified\n",
        "\n",
        "# Function to plot the profiles of proteins in a given cluster\n",
        "def plot_protein_profiles(cluster_number, final_mem_table, benchmark_modified):\n",
        "    # Find all proteins with the specified cluster number in final_mem_table\n",
        "    cluster_proteins = final_mem_table[final_mem_table['clust_id'] == cluster_number]['identifier'].tolist()\n",
        "\n",
        "    if not cluster_proteins:\n",
        "        print(f\"No proteins found in cluster {cluster_number}.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Proteins in cluster {cluster_number}: {cluster_proteins}\")\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    traces = []\n",
        "\n",
        "    for protein in cluster_proteins:\n",
        "        # Find the protein in the Abundances_modified table by matching exact identifiers or parts of grouped identifiers\n",
        "        benchmark_row = benchmark_modified[benchmark_modified['Protein IDs'].apply(lambda x: any(p in x.split(';') for p in [protein]))]\n",
        "\n",
        "        if not benchmark_row.empty:\n",
        "            # Extract the x-axis (column headers) and y-axis (values in the row for the protein)\n",
        "            x_values = benchmark_modified.columns[1:]  # X-axis: all columns except the first (Protein IDs)\n",
        "            y_values = benchmark_row.iloc[0, 1:].values  # Y-axis: corresponding values for the protein\n",
        "\n",
        "            # Create a trace for the protein's profile\n",
        "            trace = go.Scatter(\n",
        "                x=x_values,\n",
        "                y=y_values,\n",
        "                mode='lines+markers',\n",
        "                name=benchmark_row.iloc[0, 0]  # Full name from the Abundances file for better labeling\n",
        "            )\n",
        "            traces.append(trace)\n",
        "\n",
        "    # If there are any traces (proteins found in Abundances_modified), plot them\n",
        "    if traces:\n",
        "        layout = go.Layout(\n",
        "            title=f'Protein Profiles for Cluster {cluster_number}',\n",
        "            xaxis=dict(title='Benchmark Columns (X-axis)'),\n",
        "            yaxis=dict(title='Values (Y-axis)'),\n",
        "        )\n",
        "\n",
        "        fig = go.Figure(data=traces, layout=layout)\n",
        "        fig.show()  # Display the plot directly in the notebook\n",
        "    else:\n",
        "        print(f\"No proteins from cluster {cluster_number} found in Abundances_modified.tsv.\")\n",
        "\n",
        "# Main function to execute the code\n",
        "def main():\n",
        "    # Load the data\n",
        "    final_mem_table, benchmark_modified = load_data()\n",
        "\n",
        "    # Get the cluster number from the user\n",
        "    cluster_number = int(input(\"Enter the cluster number: \"))\n",
        "\n",
        "    # Plot the protein profiles for the given cluster\n",
        "    plot_protein_profiles(cluster_number, final_mem_table, benchmark_modified)\n",
        "\n",
        "# Run the main function\n",
        "main()\n"
      ],
      "metadata": {
        "id": "GGNUrr1HtnBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cluster Analysis**"
      ],
      "metadata": {
        "id": "kxLTqQeLXSX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 11. If you have an excel workbook where each complex has a seprate sheet\n",
        "and each protein in the complex is in a different row (each protein may have different identifiers,)\n",
        "then this code block lets you track the GIP identified interactors of each of those proteins in a complex\n",
        "by listing them in front of each protein in a separate, new column\n",
        "'''\n",
        "import pandas as pd\n",
        "\n",
        "# Function to load the Excel workbook and mem table TSV\n",
        "def load_data():\n",
        "    # Load the Excel workbook (Complexes.xlsx) with all its sheets\n",
        "    workbook = pd.ExcelFile('Input/Complexes.xlsx')\n",
        "\n",
        "    # Load the final mem table TSV file\n",
        "    mem_table = pd.read_csv('final_mem_table.tsv', sep='\\t')\n",
        "\n",
        "    return workbook, mem_table\n",
        "\n",
        "# Function to display available sheets and allow user to select one\n",
        "def select_sheet(workbook):\n",
        "    sheets = workbook.sheet_names\n",
        "    print(\"Available sheets:\")\n",
        "    for idx, sheet in enumerate(sheets):\n",
        "        print(f\"{idx + 1}: {sheet}\")\n",
        "\n",
        "    sheet_index = int(input(\"\\nSelect the sheet by number: \")) - 1\n",
        "    selected_sheet_name = sheets[sheet_index]\n",
        "    selected_sheet = workbook.parse(selected_sheet_name)\n",
        "    return selected_sheet, selected_sheet_name\n",
        "\n",
        "# Function to display available columns and allow user to select one\n",
        "def select_column(df):\n",
        "    print(\"\\nAvailable columns:\")\n",
        "    for idx, col in enumerate(df.columns):\n",
        "        print(f\"{idx + 1}: {col}\")\n",
        "\n",
        "    col_index = int(input(\"\\nSelect the column by number: \")) - 1\n",
        "    selected_column = df.columns[col_index]\n",
        "    return selected_column\n",
        "\n",
        "# Function to add a new column with proteins from the same cluster\n",
        "def add_cluster_info(selected_sheet, selected_column, mem_table, mem_column):\n",
        "    # Create a new column to store the cluster proteins\n",
        "    cluster_protein_col = []\n",
        "\n",
        "    for protein in selected_sheet[selected_column]:\n",
        "        # Find the cluster for the protein in the mem table\n",
        "        matching_row = mem_table[mem_table[mem_column].apply(lambda x: any(p == x for p in protein.split(';')))]\n",
        "\n",
        "        if not matching_row.empty:\n",
        "            cluster_number = matching_row.iloc[0]['clust_id']\n",
        "            # Find all proteins in that cluster\n",
        "            cluster_proteins = mem_table[mem_table['clust_id'] == cluster_number][mem_column].tolist()\n",
        "            # Create a comma-separated string of all proteins in the cluster\n",
        "            cluster_protein_col.append(', '.join(cluster_proteins))\n",
        "        else:\n",
        "            cluster_protein_col.append('')\n",
        "\n",
        "    # Add the new column to the selected sheet\n",
        "    selected_sheet['Cluster Proteins'] = cluster_protein_col\n",
        "\n",
        "    return selected_sheet\n",
        "\n",
        "# Main function to execute the code\n",
        "def main():\n",
        "    # Load data\n",
        "    workbook, mem_table = load_data()\n",
        "\n",
        "    # Step 1: Select the sheet from Complexes.xlsx\n",
        "    selected_sheet, selected_sheet_name = select_sheet(workbook)\n",
        "\n",
        "    # Step 2: Select the column with protein identifiers in the selected sheet\n",
        "    selected_column = select_column(selected_sheet)\n",
        "\n",
        "    # Step 3: Select the column with protein identifiers in the mem table\n",
        "    mem_column = select_column(mem_table)\n",
        "\n",
        "    # Step 4: Add the cluster information as a new column\n",
        "    modified_sheet = add_cluster_info(selected_sheet, selected_column, mem_table, mem_column)\n",
        "\n",
        "    # Step 5: Save the modified sheet back to the workbook\n",
        "    with pd.ExcelWriter('Input/Complexes_modified.xlsx', engine='openpyxl') as writer:\n",
        "        # Write all original sheets back to the workbook, including the modified one\n",
        "        for sheet_name in workbook.sheet_names:\n",
        "            sheet_df = workbook.parse(sheet_name)\n",
        "            # Check if this is the selected sheet, write the modified sheet\n",
        "            if sheet_name == selected_sheet_name:\n",
        "                modified_sheet.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "            else:\n",
        "                sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "    print(f\"Modified sheet saved as Complexes_modified.xlsx\")\n",
        "\n",
        "# Run the main function\n",
        "main()\n"
      ],
      "metadata": {
        "id": "TWhGBStUwuS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Vizulaize the protein wise interactor network as a 3D network below\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import plotly.graph_objs as go\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt  # For the viridis colormap\n",
        "\n",
        "# Load your Excel file\n",
        "excel_file = 'Input/Complexes_modified.xlsx'  # Update with your file\n",
        "\n",
        "# Load the workbook and list available sheets\n",
        "xls = pd.ExcelFile(excel_file)\n",
        "print(\"Available sheets:\")\n",
        "for i, sheet in enumerate(xls.sheet_names):\n",
        "    print(f\"{i + 1}. {sheet}\")\n",
        "\n",
        "# Ask the user to pick a sheet by number\n",
        "sheet_choice = int(input(\"Pick the sheet number: \")) - 1\n",
        "sheet_name = xls.sheet_names[sheet_choice]\n",
        "\n",
        "# Load the selected sheet\n",
        "df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
        "\n",
        "# List available columns\n",
        "print(\"\\nAvailable columns:\")\n",
        "for i, col in enumerate(df.columns):\n",
        "    print(f\"{i + 1}. {col}\")\n",
        "\n",
        "# Ask the user to pick the column for Proteins (source identifiers)\n",
        "protein_col_choice = int(input(\"Pick the column number for Proteins (source identifiers): \")) - 1\n",
        "protein_col = df.columns[protein_col_choice]\n",
        "\n",
        "# The Cluster Proteins column is fixed\n",
        "cluster_col = \"Cluster Proteins\"\n",
        "\n",
        "# Create a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes from the Proteins column\n",
        "for protein in df[protein_col]:\n",
        "    G.add_node(protein)\n",
        "\n",
        "# Add edges from the Cluster Proteins column\n",
        "for index, row in df.iterrows():\n",
        "    protein = row[protein_col]\n",
        "    if pd.notna(row[cluster_col]):\n",
        "        connections = str(row[cluster_col]).split(', ')\n",
        "        for connection in connections:\n",
        "            G.add_edge(protein, connection)\n",
        "\n",
        "# Use a spring layout (force-directed layout) in 3D to spread the clusters\n",
        "pos = nx.spring_layout(G, dim=3, seed=42)  # Seed for reproducibility\n",
        "\n",
        "# Create a 3D scatter plot for nodes\n",
        "x_nodes = [pos[node][0] for node in G.nodes]\n",
        "y_nodes = [pos[node][1] for node in G.nodes]\n",
        "z_nodes = [pos[node][2] for node in G.nodes]\n",
        "\n",
        "# Calculate node degrees (number of edges) to use for coloring\n",
        "degrees = dict(G.degree)\n",
        "degree_values = [degrees[node] for node in G.nodes]\n",
        "\n",
        "# Normalize the degrees for the viridis colormap\n",
        "cmap = plt.get_cmap('viridis')\n",
        "norm_degrees = np.array(degree_values) / max(degree_values)\n",
        "node_colors = [cmap(degree) for degree in norm_degrees]\n",
        "\n",
        "# Create edges for the graph\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "edge_z = []\n",
        "for edge in G.edges:\n",
        "    edge_x.extend([pos[edge[0]][0], pos[edge[1]][0], None])  # Adding None to break the line between edges\n",
        "    edge_y.extend([pos[edge[0]][1], pos[edge[1]][1], None])\n",
        "    edge_z.extend([pos[edge[0]][2], pos[edge[1]][2], None])\n",
        "\n",
        "# Create scatter plot for the nodes with coloring based on the degree (connectivity)\n",
        "node_trace = go.Scatter3d(\n",
        "    x=x_nodes,\n",
        "    y=y_nodes,\n",
        "    z=z_nodes,\n",
        "    mode='markers+text',\n",
        "    text=list(G.nodes),\n",
        "    textposition=\"top center\",\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        size=6,\n",
        "        color=[f'rgb({r*255}, {g*255}, {b*255})' for r, g, b, _ in node_colors],  # Convert RGBA to RGB for Plotly\n",
        "        line=dict(width=2)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create a line plot for edges\n",
        "edge_trace = go.Scatter3d(\n",
        "    x=edge_x,\n",
        "    y=edge_y,\n",
        "    z=edge_z,\n",
        "    mode='lines',\n",
        "    line=dict(color='black', width=2),\n",
        "    hoverinfo='none'\n",
        ")\n",
        "\n",
        "# Create a layout for the 3D plot\n",
        "layout = go.Layout(\n",
        "    title='3D Protein Interaction Network (with Cluster Spread & Degree-Based Colors)',\n",
        "    showlegend=False,\n",
        "    scene=dict(\n",
        "        xaxis=dict(showbackground=False),\n",
        "        yaxis=dict(showbackground=False),\n",
        "        zaxis=dict(showbackground=False)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Plot the figure\n",
        "fig = go.Figure(data=[edge_trace, node_trace], layout=layout)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "NcuF6rzlA9hS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
